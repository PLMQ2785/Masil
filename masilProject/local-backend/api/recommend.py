import traceback
from typing import Any, Dict, List, Optional
from uuid import UUID
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from supabase import create_client, Client
from openai import OpenAI
import json
from datetime import date

from core.config import settings
from models.schemas import RecommendRequest
from services.geo import haversine_km, estimate_travel_min
from services.time_calculator import compute_time_overlap_metrics, format_work_days
from services.recommend_calculator import compute_pay_norm
from services.recommend_calculator import calculate_final_score

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase: Client = create_client(settings.SUPABASE_URL, settings.SUPABASE_SERVICE_KEY)
client = OpenAI(api_key=settings.OPENAI_API_KEY)
router = APIRouter()

# --- RAG íŒŒì´í”„ë¼ì¸ ë° í—¬í¼ í•¨ìˆ˜ ---

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ê°€ ---
def calculate_age(birthdate_str: str) -> Optional[int]:
    if not birthdate_str: return None
    try:
        birthdate = date.fromisoformat(birthdate_str)
        today = date.today()
        # ìƒì¼ì´ ì§€ë‚¬ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë°˜ì˜í•˜ì—¬ ì •í™•í•œ ë§Œ ë‚˜ì´ ê³„ì‚°
        age = today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))
        return age
    except (ValueError, TypeError):
        return None

def format_availability(availability_json) -> str:
    if not availability_json or not isinstance(availability_json, dict):
        return "ì •ë³´ ì—†ìŒ"
    
    available_days = []
    for day, slots in availability_json.items():
        if slots: # ì‹œê°„ëŒ€ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´
            available_days.append(day)
    
    return ', '.join(available_days) if available_days else "ì •ë³´ ì—†ìŒ"

def build_prompt_for_reason(candidate, user_info, query):
    prompt = f"""ë‹¹ì‹ ì€ AI ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” '{query}'ë¼ê³  ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ [ì¼ìë¦¬ ì •ë³´]ë¥¼ ë³´ê³ , ì´ ì¼ìë¦¬ê°€ ì™œ ì‚¬ìš©ìì—ê²Œ ì¢‹ì€ ì¶”ì²œì¸ì§€ ê·¸ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                ê·¸ë¦¬ê³  ë‹µë³€ì— ê°„ë‹¨í•œ ì´ìœ , ì´ë™ì‹œê°„, ì‹œê°„ ê²¹ì¹¨ ë¹„ìœ¨, ì„ê¸ˆ ë¶„ìœ„ ì—¬ë¶€ ë“±ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì‹œì˜¤.
                <ì˜ˆì‹œ>
                ì‹¤ë‚´Â·ê°€ë²¼ì›€ì— ì í•©í•˜ê³ , ì´ë™ 17ë¶„, ì‹œê°„ ê²¹ì¹¨ 14%, ì„ê¸ˆ ì§€ì—­ ìƒìœ„ 30%ì…ë‹ˆë‹¤.
                </ì˜ˆì‹œ>

                [ì¼ìë¦¬ ì •ë³´]
                - ì œëª©: {candidate.get('title')}
                - ë‚´ìš©: {candidate.get('description')}
                - ì¥ì†Œ: {candidate.get('place')}
                - ì‹œê¸‰: {candidate.get('hourly_wage')}ì›
                - ê±°ë¦¬: {candidate.get('distance_km')}km"""
    return prompt

def generate_fallback_reason(candidate):
    return f"'{candidate.get('title')}'ì€(ëŠ”) ì‚¬ìš©ìë‹˜ì˜ ìš”ì²­ê³¼ ê´€ë ¨ì„±ì´ ë†’ì•„ ì¶”ì²œí•©ë‹ˆë‹¤."

SERVICE_AREAS = ["ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë™êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ì†¡íŒŒêµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬"]
# ë” ìœ ì—°í•œ ë¹„êµë¥¼ ìœ„í•œ í•µì‹¬ í‚¤ì›Œë“œ ëª©ë¡
SERVICE_AREA_KEYWORDS = ["ê°•ë™", "ì†¡íŒŒ", "ê°•ë‚¨"] 

def run_rag_pipeline(user_id: UUID, query: str, k: int, exclude_ids: Optional[List[int]] = None, current_latitude: Optional[float] = None, current_longitude: Optional[float] = None) -> dict:

    print("RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    # --- ğŸ‘‡ 0ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì§€ì—­ ì¶”ì¶œ ë° ê²€ì‚¬ ---
    try:
        # LLMì—ê²Œ ì§€ì—­ëª… ì¶”ì¶œì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸
        location_extraction_prompt = f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ 'ì§€ì—­ëª…'ì´ë‚˜ 'ë„ì‹œ ì´ë¦„'ì„ ëª¨ë‘ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
        ë§Œì•½ ì§€ì—­ëª…ì´ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ë‹¤ë©´, "ì—†ìŒ"ì´ë¼ê³ ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ë‹µë³€ì—ëŠ” ì˜¤ì§ ì§€ì—­ëª…ë§Œ í¬í•¨í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ë¶™ì´ì§€ ë§ˆì„¸ìš”.

        ì‚¬ìš©ì ì§ˆë¬¸: "{query}"
        ì¶”ì¶œëœ ì§€ì—­ëª…:
        """
        location_response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": location_extraction_prompt}]
        )
        extracted_location = location_response.choices[0].message.content.strip()

        print(f"ì¶”ì¶œëœ ì§€ì—­ëª…: {extracted_location}")

        # ì¶”ì¶œëœ ì§€ì—­ì´ ì„œë¹„ìŠ¤ ì§€ì—­ ë°–ì¸ì§€ í™•ì¸
        is_out_of_service = False
        if extracted_location and extracted_location != "ì—†ìŒ":
            # SERVICE_AREASì— ì¶”ì¶œëœ ì§€ì—­ëª…ì´ í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ True
            # --- ğŸ‘‡ í•µì‹¬ ìˆ˜ì • ì‚¬í•­: í‚¤ì›Œë“œë¡œ í¬í•¨ ì—¬ë¶€ í™•ì¸ ---
            if not any(extracted_location in area for area in SERVICE_AREAS):
            # --- ğŸ‘† ìˆ˜ì • ë ğŸ‘† ---
                is_out_of_service = True

        # --- ğŸ‘‡ 3ë‹¨ê³„: ì„œë¹„ìŠ¤ ì§€ì—­ ì™¸ ìš”ì²­ ì²˜ë¦¬ ---
        if is_out_of_service:
            print(f"--- ì„œë¹„ìŠ¤ ì§€ì—­ ì™¸ ìš”ì²­ ê°ì§€: {extracted_location} ---")
            
            # ì„œë¹„ìŠ¤ ë¶ˆê°€ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ LLMìœ¼ë¡œ ìƒì„±
            out_of_service_prompt = f"""
            ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ì„œë¹„ìŠ¤ ì •ì±…ì„ ì¹œì ˆí•˜ê²Œ ì•ˆë‚´í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.
            ì‚¬ìš©ìê°€ ì„œë¹„ìŠ¤ ì§€ì—­ì´ ì•„ë‹Œ '{extracted_location}'ì˜ ì¼ìë¦¬ë¥¼ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
            í˜„ì¬ ì„œë¹„ìŠ¤ëŠ” '{', '.join(SERVICE_AREAS)}' ì§€ì—­ë§Œ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì„ ì •ì¤‘í•˜ê²Œ ì„¤ëª…í•˜ê³ ,
            í–¥í›„ ì„œë¹„ìŠ¤ ì§€ì—­ í™•ëŒ€ë¥¼ ìœ„í•´ ë…¸ë ¥í•˜ê² ë‹¤ëŠ” ë©”ì‹œì§€ë¥¼ ë‹´ì•„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
            """
            response = client.chat.completions.create(
                model="gpt-5-nano",
                messages=[{"role": "user", "content": out_of_service_prompt}]
            )
            answer = response.choices[0].message.content
            
            # ì¶”ì²œ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨í•˜ê³  ì•ˆë‚´ ë©”ì‹œì§€ë§Œ ë°˜í™˜
            return {"answer": answer, "jobs": []}

    except Exception as e:
        print(f"--- ì§€ì—­ ê²€ì‚¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e} ---")
        # ì´ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì „ì²´ ì¶”ì²œì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ ê³„ì† ì§„í–‰
    
    # --- ğŸ‘† ì§€ì—­ ê²€ì‚¬ ë¡œì§ ë ğŸ‘† ---
    
    
    ab_test_flag = "llm"

    # 1. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë° íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    user_response = supabase.from_("users").select("*").eq("id", str(user_id)).single().execute()
    user_ctx = user_response.data
    if not user_ctx:
        raise HTTPException(status_code=404, detail="ì‚¬ìš©ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # DBì—ì„œ ê°€ì ¸ì˜¨ JSON ë¬¸ìì—´ì„ Python ê°ì²´ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    for key in ["preferred_jobs", "interests", "availability_json"]:
        value = user_ctx.get(key)
        if isinstance(value, str):
            try:
                # JSON íŒŒì‹± ì‹œë„
                user_ctx[key] = json.loads(value)
            except json.JSONDecodeError:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì˜ˆë¹„ ì²˜ë¦¬)
                print(f"Warning: '{key}' í•„ë“œê°€ ìœ íš¨í•œ JSONì´ ì•„ë‹ˆë¯€ë¡œ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤: {value}")
                user_ctx[key] = [item.strip() for item in value.split(',')]
    # --- ğŸ‘† ìˆ˜ì • ë ---
    
    # --- ğŸ‘‡ 1ë‹¨ê³„: ì¿¼ë¦¬ ì¬ì‘ì„± (Query Rewriting) - ì‹ ê·œ ì¶”ê°€ ---
    rewrite_prompt = f"""
        ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì‚¬ìš©ìì˜ ì¼ìë¦¬ ê²€ìƒ‰ì–´ë¥¼ ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì¬ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ê³¼ [ì‚¬ìš©ì í”„ë¡œí•„]ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬, ì‚¬ìš©ìì˜ ìˆ¨ì€ ì˜ë„ê¹Œì§€ íŒŒì•…í•œ êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

        [ì‚¬ìš©ì í”„ë¡œí•„]
        - ë‚˜ì´: {calculate_age(user_ctx.get('date_of_birth'))}ì„¸
        - ì„ í˜¸ ì§ë¬´: {user_ctx.get('preferred_jobs')}
        - ì„ í˜¸ í™˜ê²½: {user_ctx.get('preferred_environment')}
        - ê·¼ë¬´ ê°€ëŠ¥ ìš”ì¼: {format_availability(user_ctx.get('availability_json'))}

        [ì‹¤ì œ ì¬ì‘ì„± ìš”ì²­]
        - ì‚¬ìš©ì ì§ˆë¬¸: {query}
        - ì¬ì‘ì„±ëœ ì¿¼ë¦¬:
        """
    
    rewrite_response = client.chat.completions.create(
        model="gpt-5-mini", # ì¬ì‘ì„±ì€ ê°€ë²¼ìš´ ëª¨ë¸ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤.
        messages=[{"role": "user", "content": rewrite_prompt}],
    )
    
    rewritten_query = rewrite_response.choices[0].message.content.strip()
    print(f"--- ì¿¼ë¦¬ ì¬ì‘ì„± ì™„ë£Œ ---\nì›ë³¸: {query}\nì¬ì‘ì„±: {rewritten_query}\n-------------------------")
    query = rewritten_query  # ì¬ì‘ì„±ëœ ì¿¼ë¦¬ë¡œ ì—…ë°ì´íŠ¸
    # --- ğŸ‘† ì‹ ê·œ ë‹¨ê³„ ë ---
    
    
    history_response = supabase.from_("user_job_reviews").select("job_id, status").eq("user_id", str(user_id)).execute()
    user_history = history_response.data or []
    accepted_ids = {item['job_id'] for item in user_history if item['status'] in ['applied', 'completed', 'saved']}
    rejected_ids = {item['job_id'] for item in user_history if item['status'] in ['rejected']}

    # 2. ì¿¼ë¦¬ ì„ë² ë”©

    
     # --- ğŸ‘‡ 2. ì¿¼ë¦¬ ì„ë² ë”© (ìˆ˜ì •ëœ ë¶€ë¶„) ---
     
         # ë‚˜ì´ ê³„ì‚°
    age = calculate_age(user_ctx.get('date_of_birth'))
    
    # ìˆ«ì/JSON ì½”ë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (LLMì´ ì´í•´í•˜ê¸° ì‰½ë„ë¡)
    ability_map = {1: 'ìƒ', 2: 'ì¤‘', 3: 'í•˜'}
    ability_text = ability_map.get(user_ctx.get('ability_physical'))
    availability_summary = format_availability(user_ctx.get('availability_json'))
     
    # 2a. ì„ë² ë”©ì„ ìœ„í•œ ì¢…í•© í…ìŠ¤íŠ¸ ìƒì„±
    profile_info = f"""
        - ë‚˜ì´: {f'{age}ì„¸' if age else 'ì •ë³´ ì—†ìŒ'}
        - ì£¼ì†Œ: {user_ctx.get('home_address') or 'ì •ë³´ ì—†ìŒ'}
        - ê·¼ë¬´ ê°€ëŠ¥ ìš”ì¼: {availability_summary}
        - ì‹ ì²´ ëŠ¥ë ¥ ìˆ˜ì¤€: {ability_text or 'ì •ë³´ ì—†ìŒ'}
        - ì„ í˜¸ í™˜ê²½: {user_ctx.get('preferred_environment') or 'ë¬´ê´€'}
        - ìµœëŒ€ ì´ë™ ê°€ëŠ¥ ì‹œê°„: {f"{user_ctx.get('max_travel_time_min')}ë¶„" if user_ctx.get('max_travel_time_min') else 'ì •ë³´ ì—†ìŒ'}
        - ì„ í˜¸ ì§ë¬´: {', '.join(user_ctx.get('preferred_jobs') or [])}
        - ê´€ì‹¬ì‚¬: {', '.join(user_ctx.get('interests') or [])}
        - ê³¼ê±° ê²½í—˜: {user_ctx.get('work_history') or 'ì—†ìŒ'}
            """
    
    # ê¸ì •ì ì´ì—ˆë˜ í™œë™ì˜ ì œëª©ì„ ê°€ì ¸ì™€ íˆìŠ¤í† ë¦¬ ì •ë³´ êµ¬ì„± (ì„ íƒì‚¬í•­ì´ì§€ë§Œ íš¨ê³¼ì )
    if accepted_ids:
        # 1. select êµ¬ë¬¸ì— 'job_id'ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        accepted_jobs_response = supabase.from_("jobs").select("job_id, title").in_("job_id", list(accepted_ids)).execute()
        
        # 2. LLMì´ ì´í•´í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤. (ì˜ˆ: "[123] ì‹œë‹ˆì–´ ë³µì§€ ë³´ì•ˆê´€")
        accepted_job_texts = [
            f"[{job['job_id']}] {job['title']}" 
            for job in accepted_jobs_response.data
        ]
        history_info = f"- ê³¼ê±° ê¸ì •ì  í™œë™: {', '.join(accepted_job_texts)}"
    else:
        history_info = ""
        

        
    # ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©
    composite_text_for_embedding = f"""
        [ì‚¬ìš©ì ì§ˆë¬¸]
        {query}

        [ì‚¬ìš©ì í”„ë¡œí•„]
        {profile_info}

        [ê³¼ê±° í™œë™]
        {history_info}
        """
    print("--- ì„ë² ë”© ìƒì„±ìš© ì¢…í•© í…ìŠ¤íŠ¸ ---")
    print(composite_text_for_embedding)
    print("---------------------------------")
        
    # embedding_response = client.embeddings.create(input=[query], model="text-embedding-3-small")
    # query_embedding = embedding_response.data[0].embedding
    
    # 2b. ì¢…í•© í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©
    embedding_response = client.embeddings.create(
        input=[composite_text_for_embedding], # ğŸ‘ˆ query ëŒ€ì‹  composite_text_for_embedding ì‚¬ìš©
        model="text-embedding-3-small"
    )
    query_embedding = embedding_response.data[0].embedding

    # 3. í›„ë³´êµ° ê²€ìƒ‰ (Retrieval)
    candidates_response = supabase.rpc('match_jobs', {'query_embedding': query_embedding, 'match_threshold': 0.3, 'match_count': 150}).execute()
    retrieved_jobs = candidates_response.data


    if not retrieved_jobs:
        return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ìš”ì²­ê³¼ ìœ ì‚¬í•œ ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}
    
    print(f"Excluded List : {exclude_ids}")
    
    if exclude_ids:
        retrieved_jobs = [job for job in retrieved_jobs if int(job['job_id']) not in exclude_ids]
    
    if not retrieved_jobs:
        return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ë” ì´ìƒ ì¶”ì²œí•´ë“œë¦´ ë‹¤ë¥¸ ì†Œì¼ê±°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.", "jobs": []}

    retrieved_ids = [job['job_id'] for job in retrieved_jobs]
    similarity_map = {job['job_id']: job['similarity'] for job in retrieved_jobs}
    select_query = (
    "job_id, title, participants, hourly_wage, place, address, work_days, "
    "start_time, end_time, client, description, job_latitude, job_longitude, "
    "created_at, updated_at, current_participants"
    )
    # full_candidates_response = supabase.from_("jobs").select("*").in_("job_id", retrieved_ids).execute()
    full_candidates_response = supabase.from_("jobs").select(select_query).in_("job_id", retrieved_ids).execute()
    candidates = full_candidates_response.data
    
    # --- ğŸ‘‡ ê°•ë ¥í•œ í•„í„°(Hard Filter) ì¶”ê°€ ---
    original_candidate_count = len(candidates)
    
    # ì˜ˆì‹œ: ì¬ì‘ì„±ëœ ì¿¼ë¦¬ì— 'ì£¼ì¤‘'ì´ í¬í•¨ë˜ë©´, ì£¼ë§ ê·¼ë¬´ ì¼ìë¦¬ ì œì™¸
    if 'ì£¼ì¤‘' in query and ('ì£¼ë§' not in query):
        candidates = [
            job for job in candidates 
            if job.get('work_days') and (job['work_days'][5] == '0' and job['work_days'][6] == '0')
        ]
    
    # ì˜ˆì‹œ: ì¬ì‘ì„±ëœ ì¿¼ë¦¬ì— 'ì‹¤ë‚´'ê°€ í¬í•¨ë˜ë©´, ì œëª©ì´ë‚˜ ì„¤ëª…ì— 'ì‹¤ì™¸','ì•¼ì™¸'ê°€ ìˆëŠ” ì¼ìë¦¬ ì œì™¸
    if 'ì‹¤ë‚´' in query and ('ì‹¤ì™¸' not in query):
        candidates = [
            job for job in candidates
            if 'ì‹¤ì™¸' not in job.get('title','') and 'ì•¼ì™¸' not in job.get('title','') and \
               'ì‹¤ì™¸' not in job.get('description','') and 'ì•¼ì™¸' not in job.get('description','')
        ]

    print(f"--- ê°•ë ¥í•œ í•„í„° ì ìš©: {original_candidate_count}ê°œ -> {len(candidates)}ê°œ í›„ë³´ ---")
    # --- ğŸ‘† í•„í„° ì¶”ê°€ ë ğŸ‘† ---

    # 4. í•„í„°ë§ ë° ì¬ì •ë ¬ (Reranking)
    
    if ab_test_flag == "llm":    
            print("--- `LLM` ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° ì‹¤í–‰ (Chunking ë°©ì‹) ---")

            score_map = {}
            chunk_size = 30  # í•œ ë²ˆì— ì²˜ë¦¬í•  í›„ë³´ ìˆ˜ (20~30ê°œê°€ ì ë‹¹)

            # LLMì—ê²Œ ì „ë‹¬í•  í›„ë³´êµ° ì •ë³´ êµ¬ì„±
            # 1. LLM í˜¸ì¶œì„ ë°˜ë³µë¬¸ ë°–ì—ì„œ ë”± í•œ ë²ˆë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
            # candidates_for_prompt = [
            #     {key: value for key, value in job.items() if key != 'embedding'}
            #     for job in candidates
            # ]
            
            # --- ğŸ‘‡ ì²­í‚¹(Chunking) ë¡œì§ ğŸ‘‡ ---
            for i in range(0, len(candidates), chunk_size):
                # 1. ì „ì²´ í›„ë³´ ëª©ë¡ì„ ì‘ì€ ë©ì–´ë¦¬(chunk)ë¡œ ìë¦…ë‹ˆë‹¤.
                chunk = candidates[i:i + chunk_size]
                print(f"--- Chunk {i//chunk_size + 1} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ í•­ëª©) ---")

                candidates_for_prompt = [
                    {key: value for key, value in job.items() if key != 'embedding'}
                    for job in chunk
                ]
            
            # candidates_for_prompt = []
            # for job in candidates:
            #     job_info = {key: value for key, value in job.items() if key != 'embedding'}
            #     job_info['work_days_text'] = format_work_days(job.get('work_days'))
            #     candidates_for_prompt.append(job_info)
            
            
            # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ì„¤ê³„ (ì ìˆ˜ ê³„ì‚° ì—­í•  ëª…ì‹œ)
            # --- ğŸ‘‡ í”„ë¡¬í”„íŠ¸ ê°•í™” ---
                scoring_prompt = f"""
                    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í”„ë¡œí•„ê³¼ ì„ í˜¸ë„ì— ë§ì¶° ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” ìµœê³ ì˜ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    [ì‚¬ìš©ì ì •ë³´]ì™€ [ì¼ìë¦¬ í›„ë³´ ëª©ë¡]ì„ ì£¼ì˜ ê¹Šê²Œ ì½ê³ , ê° ì¼ìë¦¬ê°€ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ì–¼ë§ˆë‚˜ ì í•©í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

                    [ì—­í• ]
                    1. [ì¼ìë¦¬ í›„ë³´ ëª©ë¡]ì— ìˆëŠ” **ëª¨ë“  ì¼ìë¦¬ ê°ê°**ì— ëŒ€í•´, ì‚¬ìš©ìì™€ì˜ ì í•©ë„ë¥¼ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ 'match_score'ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
                    2. ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë” ì í•©í•˜ë©°, **ì¡°ê±´ì— ë§ì§€ ì•ŠëŠ”ë‹¤ê³  ìƒê°ë˜ë©´ ë°˜ë“œì‹œ ë‚®ì€ ì ìˆ˜(ì˜ˆ: 0.1)ë¥¼ ë¶€ì—¬**í•´ì•¼ í•©ë‹ˆë‹¤.
                    3. ëª¨ë“  í›„ë³´ì— ëŒ€í•œ í‰ê°€ ì ìˆ˜ë¥¼ ì•„ë˜ [ì¶œë ¥ í˜•ì‹]ê³¼ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ëŠ” ë‹¨ì¼ JSON ê°ì²´ë¡œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. **ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**

                    [ì‚¬ìš©ì ì •ë³´]
                    - ë‚˜ì´: {calculate_age(user_ctx.get('date_of_birth'))}ì„¸
                    - ì„ í˜¸ ì§ë¬´: {user_ctx.get('preferred_jobs')}
                    - ê´€ì‹¬ì‚¬: {user_ctx.get('interests')}
                    - ê³¼ê±° ê²½í—˜: {user_ctx.get('work_history')}

                    [ì§ˆë¬¸]
                    {query}

                    [ì¼ìë¦¬ í›„ë³´ ëª©ë¡]
                    {json.dumps(candidates_for_prompt, indent=2, ensure_ascii=False)}

                    [ì¶œë ¥ í˜•ì‹]
                    {{
                    "scores": [
                        {{
                        "job_id": <ì²« ë²ˆì§¸ job_id>,
                        "match_score": <ê³„ì‚°ëœ ì ìˆ˜>
                        }}
                    ]
                    }}
                    """
                # scoring_response = client.chat.completions.create(
                #         model="gpt-5-nano",
                #         messages=[{"role": "user", "content": scoring_prompt}],
                #         response_format={"type": "json_object"}
                #     )
                
                # raw_llm_response = scoring_response.choices[0].message.content
                
                # # --- ğŸ‘‡ ë””ë²„ê¹…ì„ ìœ„í•´ LLMì˜ ì›ë³¸ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤ ---
                # print("--- LLM Score Response (Raw) ---")
                # print(raw_llm_response)
                # print("---------------------------------")
                # # --- ğŸ‘† ë””ë²„ê¹… ì½”ë“œ ë ğŸ‘† ---
            
                # scoring_result = json.loads(scoring_response.choices[0].message.content)
                # # 2. LLMì˜ ê²°ê³¼ë¥¼ score_mapì— ì €ì¥í•´ ë‘¡ë‹ˆë‹¤.
                # # score_map = {item['job_id']: item['match_score'] for item in scoring_result.get('scores', [])}
                # # --- ğŸ‘‡ í•µì‹¬ ìˆ˜ì • ì‚¬í•­: .update() ì‚¬ìš© ---
                # # ê° ë©ì–´ë¦¬ì˜ ê²°ê³¼ë¥¼ ì „ì²´ ì ìˆ˜ ë§µ(score_map)ì— í•©ì¹©ë‹ˆë‹¤.
                # chunk_scores = {item['job_id']: item['match_score'] for item in scoring_result.get('scores', [])}
                # score_map.update(chunk_scores)
                # # --- ğŸ‘† ìˆ˜ì • ë ğŸ‘† ---
                
                try:
                    scoring_response = client.chat.completions.create(
                        model="gpt-4.1", # gpt-5-nano ëŒ€ì‹  gpt-4o ê¶Œì¥
                        messages=[{"role": "user", "content": scoring_prompt}],
                        response_format={"type": "json_object"}
                    )
                    
                    scoring_result = json.loads(scoring_response.choices[0].message.content)

                    # --- ğŸ‘‡ ë””ë²„ê¹…ì„ ìœ„í•´ LLMì˜ ì›ë³¸ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤ ---
                    print("--- LLM Score Response (Raw) ---")
                    print(scoring_result)
                    print("---------------------------------")
                    # --- ğŸ‘† ë””ë²„ê¹… ì½”ë“œ ë ğŸ‘† ---

                    # --- ğŸ‘‡ í•µì‹¬ ìˆ˜ì • ì‚¬í•­: .update() ì‚¬ìš© ---
                    # ê° ë©ì–´ë¦¬ì˜ ê²°ê³¼ë¥¼ ì „ì²´ ì ìˆ˜ ë§µ(score_map)ì— í•©ì¹©ë‹ˆë‹¤.
                    chunk_scores = {item['job_id']: item['match_score'] for item in scoring_result.get('scores', [])}
                    score_map.update(chunk_scores)
                    # --- ğŸ‘† ìˆ˜ì • ë ğŸ‘† ---

                except Exception as e:
                    print(f"--- Chunk ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ, í•´ë‹¹ ChunkëŠ” ê±´ë„ˆëœë‹ˆë‹¤: {e} ---")
                    continue
            
            print(f"--- ì „ì²´ {len(score_map)}ê°œ í•­ëª©ì— ëŒ€í•œ LLM ì ìˆ˜ ê³„ì‚° ì™„ë£Œ ---")
    
    print("--- ì¬ì •ë ¬ ì‹œì‘ ---")
    reranked_jobs = []
    
    print("--- ì„ê¸ˆí†µê³„ ê³„ì‚° ---")
    # ì§€ì—­ë³„ ì„ê¸ˆ í†µê³„ ì‚¬ì „ ê³„ì‚°
    by_place: Dict[str, List[Dict[str, Any]]] = {}
    for c in candidates:
        by_place.setdefault(c.get("place", ""), []).append(c)
    
    print("--- ì ìˆ˜ í• ë‹¹ ---")
    for job in candidates:
        base_lat = current_latitude if current_latitude is not None else user_ctx.get('home_latitude')
        base_lon = current_longitude if current_longitude is not None else user_ctx.get('home_longitude')
        
        distance_km = haversine_km(base_lat, base_lon, job.get('job_latitude'), job.get('job_longitude')) if base_lat and base_lon else None
        
        time_metrics = compute_time_overlap_metrics(user_ctx.get("availability_json", {}), job.get("work_days"), job.get("start_time"), job.get("end_time"))
        
        history_score = 1.0 if job['job_id'] in accepted_ids else -1.0 if job['job_id'] in rejected_ids else 0
        
        distance_score = (1 - (distance_km / 20)) if distance_km is not None and distance_km <= 20 else 0
        
        # pay_norm_score = compute_pay_norm(region_list, pay)

        # final_score = (
        #     similarity_map.get(job['job_id'], 0) * 0.5 +
        #     distance_score * 0.2 +
        #     time_metrics.get("time_fit", 0.0) * 0.2
        #     # pay_norm_score * 0.1
        # )
        
        # ì ìˆ˜ í• ë‹¹ (A/B ë¶„ê¸°)
        if ab_test_flag == "llm":
            # 3. score_mapì—ì„œ í•´ë‹¹ jobì˜ ì ìˆ˜ë¥¼ ì°¾ì•„ í• ë‹¹í•©ë‹ˆë‹¤.
            job['match_score'] = score_map.get(job['job_id'], 0.0)
        else:
            final_score = calculate_final_score(
                    job=job,
                    user_ctx=user_ctx,
                    similarity_map=similarity_map,
                    accepted_ids=accepted_ids,
                    rejected_ids=rejected_ids,
                    region_list=by_place.get(job.get("place", ""), []),
                    all_candidates=candidates,
                    current_latitude=base_lat,
                    current_longitude=base_lon
                )
        
            # job['match_score'] = round(final_score, 4)
            job['match_score'] = final_score['match_score']
            
        job['distance_km'] = round(distance_km, 2) if distance_km is not None else None
        job['travel_min'] = estimate_travel_min(distance_km)
        job['time_fit'] = time_metrics.get("time_fit", 0.0)
        reranked_jobs.append(job)
        
    reranked_jobs.sort(key=lambda x: x.get('match_score', 0), reverse=True)
    
    # --- ğŸ‘‡ ìµœì € ì ìˆ˜ í•„í„°ë§ ë¡œì§ ì¶”ê°€ ğŸ‘‡ ---
    
    # 2. ì ìˆ˜ê°€ 0.2ë¥¼ ì´ˆê³¼í•˜ëŠ” í•­ëª©ë§Œ ìµœì¢… í›„ë³´ë¡œ ë‚¨ê¹ë‹ˆë‹¤.
    qualified_jobs = [
        job for job in reranked_jobs if job.get('match_score', 0) > 0.2
    ]
    
    # --- ğŸ‘† ë¡œì§ ì¶”ê°€ ë ğŸ‘† ---
    
    top_k_jobs = reranked_jobs[:k]
    # top_k_jobs = qualified_jobs[:k]
    
    if not top_k_jobs:
        return {"answer": "ì¡°ê±´ì— ë§ëŠ” ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}
    
    # 5. ì¶”ì²œ ì´ìœ  ìƒì„±
    # for job in top_k_jobs:
    #     try:
    #         prompt = build_prompt_for_reason(job, user_ctx, query)
    #         reason_response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], temperature=0.2)
    #         job['reason'] = reason_response.choices[0].message.content
    #     except Exception:
    #         job['reason'] = generate_fallback_reason(job)
    
    # top_k_for_prompt = [
    #     {key: value for key, value in job.items() if key != 'embedding'}
    #     for job in top_k_jobs
    # ]
    
    print("--- ì´ìœ  ìƒì„± ---")
    # ëª¨ë“  ì¶”ì²œ ì´ìœ ë¥¼ í•œ ë²ˆì— ìƒì„±í•˜ë„ë¡ í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    top_k_for_prompt = []
    for job in top_k_jobs:
        job_info = {key: value for key, value in job.items() if key != 'embedding'}
        job_info['work_days_text'] = format_work_days(job.get('work_days'))
        top_k_for_prompt.append(job_info)
    
    reason_generation_prompt = f"""
        ë‹¹ì‹ ì€ AI ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ê³¼ [ì‚¬ìš©ì ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ [ì¶”ì²œ ì¼ìë¦¬ ëª©ë¡]ì— ìˆëŠ” ê° ì¼ìë¦¬ì— ëŒ€í•´ ì™œ ì¢‹ì€ ì¶”ì²œì¸ì§€ ê·¸ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

        [ì‚¬ìš©ì ì •ë³´]
        - ë‚˜ì´: {calculate_age(user_ctx.get('date_of_birth'))}ì„¸
        - ì„ í˜¸ ì§ë¬´: {user_ctx.get('preferred_jobs')}
        - ê´€ì‹¬ì‚¬: {user_ctx.get('interests')}

        [ì§ˆë¬¸]
        {query}

        [ì¶”ì²œ ì¼ìë¦¬ ëª©ë¡]
        {json.dumps(top_k_for_prompt, indent=2, ensure_ascii=False)}

        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. 'reasons' ë¦¬ìŠ¤íŠ¸ì—ëŠ” [ì¶”ì²œ ì¼ìë¦¬ ëª©ë¡]ê³¼ ë™ì¼í•œ ìˆœì„œë¡œ ê° job_idì™€ ì¶”ì²œ ì´ìœ ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        {{
        "reasons": [
            {{
            "job_id": <ì²« ë²ˆì§¸ job_id>,
            "reason": "<ì²« ë²ˆì§¸ ì¶”ì²œ ì´ìœ  ìš”ì•½>"
            }},
            {{
            "job_id": <ë‘ ë²ˆì§¸ job_id>,
            "reason": "<ë‘ ë²ˆì§¸ ì¶”ì²œ ì´ìœ  ìš”ì•½>"
            }}
        ]
        }}
        """
        
    try:
        # LLMì„ ë”± í•œ ë²ˆë§Œ í˜¸ì¶œí•©ë‹ˆë‹¤.
        reason_response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": reason_generation_prompt}],
            response_format={"type": "json_object"}
        )
        
        reason_result = json.loads(reason_response.choices[0].message.content)
        reason_map = {item['job_id']: item['reason'] for item in reason_result.get('reasons', [])}

        # ìƒì„±ëœ ì´ìœ ë¥¼ top_k_jobsì— ë§¤í•‘í•©ë‹ˆë‹¤.
        for job in top_k_jobs:
            job['reason'] = reason_map.get(job['job_id'], generate_fallback_reason(job))

    except Exception as e:
        print(f"--- ì¶”ì²œ ì´ìœ  ìƒì„± ì‹¤íŒ¨, í´ë°±(fallback) ë¡œì§ ì‹¤í–‰: {e} ---")
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ê° jobì— ëŒ€í•´ ê°„ë‹¨í•œ ê¸°ë³¸ ì´ìœ ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
        for job in top_k_jobs:
            job['reason'] = generate_fallback_reason(job)

    # 6. ìµœì¢… ë‹µë³€ ìƒì„±
    print("--- ìµœì¢… ë‹µë³€ ìƒì„± ---")
    context = "\n\n".join([f"- ì œëª©: {job['title']}\n- ë‚´ìš©: {job['description']}" for job in top_k_jobs])
    prompt = f"""ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì‚¬ìš©ìì—ê²Œ ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œ 'ì¡ìˆìœ¼'ì…ë‹ˆë‹¤.
                ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì•„ë˜ [ê²€ìƒ‰ëœ ì¼ìë¦¬ ì •ë³´]ì™€ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì„ ì¢…í•©í•˜ì—¬ ê°œì¸í™”ëœ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ìƒˆë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
                [ê·œì¹™]
                1. ì‚¬ëŒì—ê²Œ ë§ì„ ê±°ëŠ” ë“¯í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
                2. ê²€ìƒ‰ëœ ì •ë³´ ì¤‘ ê°€ì¥ ì¶”ì²œ ì ìˆ˜ê°€ ë†’ì€ ì¼ìë¦¬ 1~2ê°œë¥¼ ì–¸ê¸‰í•˜ë©° ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì—®ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                3. ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬(ì˜ˆ: 'ì¡°ìš©í•œ', 'ì»´í“¨í„°')ì„ ë‹µë³€ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.
                4. ìµœì¢… ë‹µë³€ì€ 2~3 ë¬¸ì¥ìœ¼ë¡œ ì™„ì„±í•˜ì„¸ìš”.
                [ê²€ìƒ‰ëœ ì¼ìë¦¬ ì •ë³´]\n{context}\n[ì§ˆë¬¸]\n{query}\n[ì¶”ì²œ ë©”ì‹œì§€]"""
    chat_response = client.chat.completions.create(model="gpt-4.1-mini", messages=[{"role": "user", "content": prompt}])
    answer = chat_response.choices[0].message.content

    return {"answer": answer, "jobs": top_k_jobs}


# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@router.post("/recommend")
def recommend_jobs_text(request: RecommendRequest):
    try:
        return run_rag_pipeline(request.user_id, request.query, 10, request.exclude_ids, request.current_latitude, request.current_longitude)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@router.post("/stt")
def speech_to_text(audio_file: UploadFile = File(...)):
    try:
        file_content = audio_file.file.read()
        mime_type = "audio/m4a" if audio_file.filename.lower().endswith('.m4a') else audio_file.content_type

        transcript_response = client.audio.transcriptions.create(
            model="whisper-1",
            file=(audio_file.filename, file_content, mime_type),
            response_format="text"
        )
        return {"text": str(transcript_response).strip()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=traceback.format_exc())

@router.post("/recommend-voice")
def recommend_jobs_voice(user_id: UUID = Form(...), audio_file: UploadFile = File(...), exclude_ids: Optional[str] = Form(None), current_latitude: Optional[float] = Form(None), current_longitude: Optional[float] = Form(None)):
    try:
        transcript_response = client.audio.transcriptions.create(
            model="whisper-1",
            file=(audio_file.filename, audio_file.file.read()),
            response_format="text"
        )
        query_text = transcript_response.strip()
        
        exclude_ids_list = [int(id_str) for id_str in exclude_ids.split(',')] if exclude_ids else []
        
        return run_rag_pipeline(user_id, query_text, 5, exclude_ids_list, current_latitude, current_longitude)
    except Exception as e:
        raise HTTPException(status_code=500, detail=traceback.format_exc())