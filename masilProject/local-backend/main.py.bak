import hashlib
import json
import math
import os
import traceback
from datetime import datetime, timedelta, timezone, date
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

import numpy as np
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from pydantic import BaseModel, Field
from supabase import Client, create_client

# --- 1. ì´ˆê¸°í™” ---
load_dotenv()

# Supabase ë° OpenAI í´ë¼ì´ì–¸íŠ¸
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# FastAPI ì•±
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "https://localhost:5173", # https ë¡œì»¬í˜¸ìŠ¤íŠ¸
        "http://192.168.68.67:5173", # http IP ì£¼ì†Œ
        "https://192.168.68.67:5173", # ğŸ‘ˆ ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”.
        "https://jobis.ngrok.app",
        "https://jobisbe.ngrok.app",
        "http://192.168.68.113:5173"
        ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 2. Pydantic ë°ì´í„° ëª¨ë¸ ---
class Job(BaseModel):
    title: str
    participants: Optional[int] = None
    hourly_wage: int
    place: str
    address: Optional[str] = None
    work_days: Optional[str] = Field(None, max_length=7)
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    client: Optional[str] = None
    description: Optional[str] = None
    job_latitude: float
    job_longitude: float

class Review(BaseModel):
    user_id: UUID
    rating: int = Field(..., ge=1, le=5)
    review_text: Optional[str] = None
    status: str

class RecommendRequest(BaseModel):
    user_id: UUID
    query: str
    exclude_ids: Optional[List[int]] = None
    current_latitude: Optional[float] = None #gps ì¢Œí‘œ
    current_longitude: Optional[float] = None #gps ì¢Œí‘œ

class ApplyRequest(BaseModel):
    user_id: UUID
    
class SessionUpdateRequest(BaseModel):
    user_id: UUID
    session_id: UUID
    
# --- Pydantic ë°ì´í„° ëª¨ë¸ ì¶”ê°€ ---
class EngagementRequest(BaseModel):
    user_id: UUID
    job_id: int
    status: str # 'applied', 'saved', 'rejected', 'dismissed' ë“±
    
class UserProfileUpdate(BaseModel):
    nickname: Optional[str] = Field(None, max_length=50)
    gender: Optional[str] = Field(None, pattern="^(M|F)$")
    date_of_birth: Optional[date] = None
    home_address: Optional[str] = Field(None, max_length=120)
    home_latitude: Optional[float] = None
    home_longitude: Optional[float] = None
    # ğŸ‘‡ --- ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•©ë‹ˆë‹¤ --- ğŸ‘‡
    preferred_jobs: Optional[List[str]] = None # Dict -> List[str]
    interests: Optional[List[str]] = None      # Dict -> List[str]
    # ğŸ‘† --- ìˆ˜ì • ë --- ğŸ‘†
    availability_json: Optional[Dict[str, Any]] = None
    work_history: Optional[str] = None
    ability_physical: Optional[int] = Field(None, ge=1, le=3)
    preferred_environment: Optional[str] = Field(None, pattern="^(indoor|outdoor|any)$")
    max_travel_time_min: Optional[int] = Field(None, gt=0)
    
# [Users Utility]
class SessionUpdateRequest(BaseModel):
    user_id: UUID
    session_id: UUID

# --- 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (AI-1, AI-2 ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê°€ì ¸ì˜´) ---
WEEKDAYS = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]

#AI Part 1
def haversine_km(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371.0088
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = phi2 - phi1
    dlmb = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlmb / 2) ** 2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return R * c

def parse_time_to_min(s: str) -> int:
    if not s or ":" not in s: return 0
    parts = s.split(":")
    return int(parts[0]) * 60 + int(parts[1])

def interval_overlap_min(a_start: int, a_end: int, b_start: int, b_end: int) -> int:
    return max(0, min(a_end, b_end) - max(a_start, b_start))

def parse_work_days(bits: str) -> List[str]:
    bits = (bits or "").strip()
    if len(bits) != 7 or not set(bits) <= {"0", "1"}: return []
    return [WEEKDAYS[i] for i, ch in enumerate(bits) if ch == "1"]

def compute_time_overlap_metrics(availability_json: Dict, work_days_bits: str, start_time: str, end_time: str) -> Dict:
    """
    Returns multiple time-fit metrics:
      - job_norm: overlapped minutes / (job daily minutes * number of job days)
      - intersection_norm: overlapped minutes / (job daily minutes * number of days where either the start day
                            or the following day has user availability; i.e., days that can actually overlap)
      - user_fit_ratio: overlapped minutes / (user total available minutes across the week)
      - time_fit: composite score combining the three (geometric mean for balance)

    Supports overnight shifts (e.g., 22:00â€“02:00), counting overlap against the start day and the next day.
    """
    
    # --- ğŸ‘‡ ì…ë ¥ê°’ None ë°©ì–´ ì½”ë“œ ì¶”ê°€ ğŸ‘‡ ---
    if not availability_json: availability_json = {}
    if not work_days_bits: work_days_bits = "0000000"
    if not start_time: start_time = "00:00:00"
    if not end_time: end_time = "00:00:00"
    # --- ğŸ‘† ìˆ˜ì • ë ğŸ‘† ---
    
    cand_days = parse_work_days(work_days_bits)
    if not cand_days:
        return {"job_norm": 0.0, "intersection_norm": 0.0, "user_fit_ratio": 0.0, "time_fit": 0.0}

    # Candidate shift minutes (per day)
    c_start = parse_time_to_min(start_time)
    c_end = parse_time_to_min(end_time)
    overnight = False
    if c_end <= c_start:
        # Treat as overnight: end on next day
        c_end += 24*60
        overnight = True
    day_sched = c_end - c_start  # per day

    # Precompute user availability per weekday in minutes and total
    def slots_minutes(slots: List[List[str]]) -> int:
        total = 0
        for slot in slots or []:
            s = parse_time_to_min(slot[0][:5]); e = parse_time_to_min(slot[1][:5])
            if e <= s:
                # Skip user overnight slots for now; recommend splitting into two slots if needed
                continue
            total += (e - s)
        return total

    user_total_min = 0
    user_min_by_day = {}
    for day in WEEKDAYS:
        mins = slots_minutes(availability_json.get(day, []))
        user_min_by_day[day] = mins
        user_total_min += mins

    # Overlap for a shift that STARTS on `day`
    def overlap_with_day(day: str) -> int:
        olap = 0
        # Segment A: [c_start, min(c_end, 1440)) on `day`
        segA_start, segA_end = c_start, min(c_end, 1440)
        if segA_end > segA_start:
            for slot in availability_json.get(day, []):
                s = parse_time_to_min(slot[0][:5]); e = parse_time_to_min(slot[1][:5])
                if e > s:
                    olap += interval_overlap_min(segA_start, segA_end, s, e)
        # Segment B: if overnight, [0, c_end-1440) on next day
        if overnight and c_end > 1440:
            next_idx = (WEEKDAYS.index(day) + 1) % 7
            next_day = WEEKDAYS[next_idx]
            segB_start, segB_end = 0, c_end - 1440
            if segB_end > segB_start:
                for slot in availability_json.get(next_day, []):
                    s = parse_time_to_min(slot[0][:5]); e = parse_time_to_min(slot[1][:5])
                    if e > s:
                        olap += interval_overlap_min(segB_start, segB_end, s, e)
        return olap

    overlap_min = 0
    intersection_days = 0
    for day in cand_days:
        overlap_d = overlap_with_day(day)
        # Cap per-day overlap by the daily schedule minutes
        overlap_min += min(overlap_d, day_sched)
        # Count "intersection-eligible" days
        next_day = WEEKDAYS[(WEEKDAYS.index(day)+1) % 7]
        if user_min_by_day.get(day, 0) > 0 or (overnight and user_min_by_day.get(next_day, 0) > 0):
            intersection_days += 1

    job_total_min = day_sched * len(cand_days)

    # Metrics
    job_norm = (overlap_min / job_total_min) if job_total_min > 0 else 0.0
    inter_den = (day_sched * max(intersection_days, 1))
    intersection_norm = (overlap_min / inter_den) if inter_den > 0 else 0.0
    user_fit_ratio = (overlap_min / user_total_min) if user_total_min > 0 else 0.0

    # Composite: geometric mean with small epsilon to avoid zero-locking
    eps = 1e-6
    time_fit = ((job_norm+eps) * (intersection_norm+eps) * (user_fit_ratio+eps)) ** (1/3) - eps

    return {
        "job_norm": round(job_norm, 2),
        "intersection_norm": round(intersection_norm, 2),
        "user_fit_ratio": round(user_fit_ratio, 2),
        "time_fit": round(time_fit, 2),
        "overlap_min": int(round(overlap_min)),
        "job_total_min": int(round(job_total_min)),
        "user_total_min": int(round(user_total_min)),
    }


def estimate_travel_min(distance_km: float) -> int:
    if distance_km <= 1.5: speed_kmh, penalty = 4.5, 0
    elif distance_km <= 10: speed_kmh, penalty = 18.0, 10
    else: speed_kmh, penalty = 30.0, 8
    return int(round((distance_km / max(speed_kmh, 1e-6)) * 60 + penalty))

#AI Part 2
def build_prompt_for_reason(candidate, user_info, query):
    """ê°œë³„ ì¶”ì²œ ì´ìœ  ìƒì„±ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸"""
    prompt = f"""ë‹¹ì‹ ì€ AI ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” '{query}'ë¼ê³  ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ [ì¼ìë¦¬ ì •ë³´]ë¥¼ ë³´ê³ , ì´ ì¼ìë¦¬ê°€ ì™œ ì‚¬ìš©ìì—ê²Œ ì¢‹ì€ ì¶”ì²œì¸ì§€ ê·¸ ì´ìœ ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì¼ìë¦¬ ì •ë³´]
- ì œëª©: {candidate.get('title')}
- ë‚´ìš©: {candidate.get('description')}
- ì¥ì†Œ: {candidate.get('place')}
- ì‹œê¸‰: {candidate.get('hourly_wage')}ì›
- ê±°ë¦¬: {candidate.get('distance_km')}km"""
    return prompt

def generate_fallback_reason(candidate):
    """LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ì¶”ì²œ ì´ìœ """
    return f"'{candidate.get('title')}'ì€(ëŠ”) ì‚¬ìš©ìë‹˜ì˜ ìš”ì²­ê³¼ ê´€ë ¨ì„±ì´ ë†’ì•„ ì¶”ì²œí•©ë‹ˆë‹¤."

# (ê¸°íƒ€ compute_time_overlap, llm_enrich_batch ë“± í•„ìš”í•œ ìœ í‹¸ í•¨ìˆ˜ë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤)

# --- 4. API ì—”ë“œí¬ì¸íŠ¸ ---

# [Jobs CRUD]
@app.post("/api/jobs")
def create_job(job: Job):
    text_to_embed = f"ì œëª©: {job.title}\në‚´ìš©: {job.description}\nì¥ì†Œ: {job.place}\ní´ë¼ì´ì–¸íŠ¸: {job.client}"
    try:
        embedding_response = client.embeddings.create(input=[text_to_embed], model="text-embedding-3-small")
        embedding_vector = embedding_response.data[0].embedding
        job_data = job.model_dump()
        job_data["embedding"] = embedding_vector
        response = supabase.from_("jobs").insert(job_data).execute()
        return response.data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/jobs")
def get_jobs(user_id: Optional[UUID] = None, limit: int = 10):
    """
    user_id ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì¶”ì²œ ëª©ë¡ì„ ë°˜í™˜í•˜ê³ ,
    ì—†ìœ¼ë©´ ê´€ë¦¬ì í˜ì´ì§€ë¥¼ ìœ„í•œ ì „ì²´ ì¼ìë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # --- 1. ê°œì¸í™” ì¶”ì²œ ë¡œì§ (user_idê°€ ìˆì„ ê²½ìš°) ---
        if user_id:
            # 1a. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (ê¸°ì¤€ ìœ„ì¹˜, ì„ í˜¸ ì§ë¬´ ë“±)
            user_response = supabase.from_("users").select(
                "home_latitude, home_longitude, preferred_jobs"
            ).eq("id", str(user_id)).single().execute()
            
            user_profile = user_response.data
            if not user_profile or user_profile.get("home_latitude") is None:
                raise HTTPException(status_code=404, detail="ì‚¬ìš©ì í”„ë¡œí•„ ë˜ëŠ” ê¸°ì¤€ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

            user_lat = user_profile["home_latitude"]
            user_lon = user_profile["home_longitude"]
            preferred_jobs = user_profile.get("preferred_jobs", [])

            # 1b. ì‚¬ìš©ìì˜ ê¸°ì¤€ ìœ„ì¹˜ ì£¼ë³€ ì¼ìë¦¬ ê²€ìƒ‰ (1ì°¨ í•„í„°ë§)
            nearby_jobs_response = supabase.rpc('nearby_jobs_full', {
                'user_lat': user_lat,
                'user_lon': user_lon,
                'radius_meters': 10000, # 10km ë°˜ê²½
                'result_limit': limit
            }).execute()
            
            nearby_jobs_data = nearby_jobs_response.data
            if not nearby_jobs_data:
                return []

            # 1c. ì¬ì •ë ¬ (Reranking): ì„ í˜¸ ì§ë¬´ì™€ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            recommended_jobs = []
            for job in nearby_jobs_data:
                title = job.get("title", "")
                
                # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°: ì„ í˜¸ ì§ë¬´ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ë©´ 1ì ì”© ì¶”ê°€
                preference_score = 0
                if preferred_jobs:
                    for pref in preferred_jobs:
                        if pref in title:
                            preference_score += 1
                
                # ê±°ë¦¬ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ)
                distance = haversine_km(user_lat, user_lon, job['job_latitude'], job['job_longitude'])
                distance_score = 1 - (distance / 10) if distance <= 10 else 0

                # ìµœì¢… ì ìˆ˜ (ì„ í˜¸ë„ 50%, ê±°ë¦¬ 50%)
                job['match_score'] = round((preference_score * 0.5) + (distance_score * 0.5), 4)
                recommended_jobs.append(job)

            # ìµœì¢… ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            recommended_jobs.sort(key=lambda x: x['match_score'], reverse=True)

            return recommended_jobs[:limit]  # ìƒìœ„ Nê°œ ë°˜í™˜

        # --- 2. ì „ì²´ ì¡°íšŒ ë¡œì§ (user_idê°€ ì—†ì„ ê²½ìš°) ---
        else:
            response = supabase.from_("jobs").select("*").order("created_at", desc=True).limit(limit).execute()
            return response.data

    except Exception as e:
        error_traceback = traceback.format_exc()
        raise HTTPException(status_code=500, detail=error_traceback)


@app.get("/api/jobs/{job_id}")
def get_job_by_id(job_id: int):
    try:
        response = supabase.from_("jobs").select("*").eq("job_id", job_id).single().execute()
        return response.data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ID {job_id} ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.put("/api/jobs/{job_id}")
def update_job(job_id: int, job: Job):
    text_to_embed = f"ì œëª©: {job.title}\në‚´ìš©: {job.description}\nì¥ì†Œ: {job.place}\ní´ë¼ì´ì–¸íŠ¸: {job.client}"
    try:
        embedding_response = client.embeddings.create(input=[text_to_embed], model="text-embedding-3-small")
        embedding_vector = embedding_response.data[0].embedding
        job_data = job.model_dump()
        job_data["embedding"] = embedding_vector
        job_data["updated_at"] = "now()"
        response = supabase.from_("jobs").update(job_data).eq("job_id", job_id).execute()
        return response.data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")

@app.delete("/api/jobs/{job_id}")
def delete_job(job_id: int):
    try:
        response = supabase.from_("jobs").delete().eq("job_id", job_id).execute()
        if not response.data:
            raise HTTPException(status_code=404, detail=f"ID {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"message": f"ID {job_id}ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

# [ì§€ì›í•˜ê¸°]
@app.post("/api/jobs/{job_id}/apply")
def apply_for_job(job_id: int, request: ApplyRequest):
    try:
        # 1. ì§€ì› ë§ˆê° ì—¬ë¶€ í™•ì¸ (ê¸°ì¡´ê³¼ ë™ì¼)
        job_response = supabase.from_("jobs").select("participants, current_participants").eq("job_id", job_id).single().execute()
        job = job_response.data
        if not job:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì¼ìë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if job.get('participants') is not None and job.get('current_participants', 0) >= job.get('participants'):
            raise HTTPException(status_code=400, detail="ëª¨ì§‘ì´ ë§ˆê°ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 2. ì‚¬ìš©ìì˜ ì§€ì› ê¸°ë¡ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
        review_data = {"job_id": job_id, "user_id": str(request.user_id), "status": "applied"}
        supabase.from_("user_job_reviews").upsert(review_data).execute()
        
        # 3. DB í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì§€ì›ì ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ 1 ì¦ê°€
        supabase.rpc('increment_applicants', {'job_id_to_update': job_id}).execute()

        return {"message": "ì§€ì›ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì› ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


# [Reviews CRUD]
@app.post("/api/jobs/{job_id}/reviews")
def create_review_for_job(job_id: int, review: Review):
    try:
        review_data = review.model_dump()
        review_data["job_id"] = job_id
        review_data["user_id"] = str(review.user_id)
        supabase.from_("user_job_reviews").insert(review_data).execute()
        
        agg_response = supabase.from_("user_job_reviews").select("rating", count="exact").eq("job_id", job_id).execute()
        ratings = [item['rating'] for item in agg_response.data if item.get('rating') is not None]
        new_review_count = agg_response.count
        new_avg_rating = sum(ratings) / len(ratings) if ratings else 0

        supabase.from_("jobs").update({
            # "average_rating": new_avg_rating,
            # "review_count": new_review_count
        }).eq("job_id", job_id).execute()
        
        return {"message": "ë¦¬ë·°ê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¦¬ë·° ë“±ë¡ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/jobs/{job_id}/reviews")
def get_reviews_for_job(job_id: int):
    try:
        response = supabase.from_("user_job_reviews").select("*").eq("job_id", job_id).execute()
        return response.data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¦¬ë·° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")




@app.post("/api/users/update-session")
def update_user_session(request: SessionUpdateRequest):
    try:
        response = supabase.from_("users").update({
            "latest_session_id": str(request.session_id)
        }).eq("id", str(request.user_id)).execute()
        if not response.data:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"message": "ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")


# [Geocoding Utility]
@app.get("/api/geocode")
def geocode_address(address: str = Query(..., min_length=1)):
    api_key_id = os.getenv('NAVER_API_KEY_ID')
    api_key = os.getenv('NAVER_API_KEY')
    if not api_key_id or not api_key: raise HTTPException(status_code=500, detail="API í‚¤ê°€ ì„œë²„ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    url = f"https://maps.apigw.ntruss.com/map-geocode/v2/geocode?query={address}"
    headers = {"X-NCP-APIGW-API-KEY-ID": api_key_id, "X-NCP-APIGW-API-KEY": api_key}
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        if data.get("status") == "OK" and data.get("addresses"):
            coords = data["addresses"][0]
            return {"latitude": float(coords["y"]), "longitude": float(coords["x"])}
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì£¼ì†Œì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Naver API í†µì‹  ì˜¤ë¥˜: {str(e)}")


# [Recommendation RAG API]
# @app.post("/api/recommend-test")
# def recommend_jobs(request: RecommendRequest):
#     """(ë™ê¸° ìµœì¢…ë³¸) ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ì¶”ì²œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
#     try:
#         # --- 1ë‹¨ê³„: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ---
#         user_response = supabase.from_("users").select("*").eq("id", str(request.user_id)).single().execute()
#         user_ctx = user_response.data
#         if not user_ctx:
#             raise HTTPException(status_code=404, detail="ì‚¬ìš©ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

#         # --- 2ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ---
#         embedding_response = client.embeddings.create(input=[request.query], model="text-embedding-3-small")
#         query_embedding = embedding_response.data[0].embedding

#         # --- 3ë‹¨ê³„: í›„ë³´êµ° ê²€ìƒ‰ (Retrieval) ---
#         candidates_response = supabase.rpc('match_jobs', {
#             'query_embedding': query_embedding,
#             'match_threshold': 0.3, # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì´ ê°’ì„ íŠœë‹í•´ì•¼ í•©ë‹ˆë‹¤.
#             'match_count': 50
#         }).execute()
        
#         retrieved_jobs = candidates_response.data
        
#         # ğŸ‘‡ --- ë””ë²„ê¹…ì„ ìœ„í•œ print ë¬¸ ì¶”ê°€ --- ğŸ‘‡
#         print(f"\n--- ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ---")
#         print(f"ìœ ì‚¬ë„ 0.3 ì´ìƒì¸ í›„ë³´ {len(retrieved_jobs)}ê°œ ë°œê²¬ë¨")
#         print("---------------------\n")
#         # ğŸ‘† --- ë””ë²„ê¹… ì½”ë“œ ë --- ğŸ‘†
        
#         if not retrieved_jobs:
#             return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ìš”ì²­ê³¼ ìœ ì‚¬í•œ ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}

#         retrieved_ids = [job['job_id'] for job in retrieved_jobs]
#         similarity_map = {job['job_id']: job['similarity'] for job in retrieved_jobs}
        
#         full_candidates_response = supabase.from_("jobs").select("*").in_("job_id", retrieved_ids).execute()
#         candidates = full_candidates_response.data

#         # --- 4ë‹¨ê³„: í•„í„°ë§ ë° ì¬ì •ë ¬ (Filtering & Reranking) ---
#         reranked_jobs = []
#         for job in candidates:
#             # ê±°ë¦¬ ê³„ì‚°
#             distance_km = haversine_km(
#                 user_ctx.get('home_latitude'), user_ctx.get('home_longitude'),
#                 job.get('job_latitude'), job.get('job_longitude')
#             )
            
#             # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì˜ë¯¸ìœ ì‚¬ë„ 70%, ê±°ë¦¬ 30%)
#             distance_score = 1 - (distance_km / 20) if distance_km <= 20 else 0 # 20kmë¥¼ ìµœëŒ€ ê±°ë¦¬ë¡œ ê°€ì •
#             match_score = similarity_map.get(job['job_id'], 0) * 0.7 + distance_score * 0.3
            
#             job['match_score'] = round(match_score, 4)
#             job['distance_km'] = round(distance_km, 2)
#             reranked_jobs.append(job)
            
#         reranked_jobs.sort(key=lambda x: x.get('match_score', 0), reverse=True)
#         top_5_jobs = reranked_jobs[:5]

#         if not top_5_jobs:
#             return {"answer": "ì¡°ê±´ì— ë§ëŠ” ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}

#         # --- 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„± (Generation) ---
#         context = "\n\n".join([f"- ì œëª©: {job['title']} (ID: {job['job_id']})\n- ë‚´ìš©: {job['description']}" for job in top_5_jobs])
#         prompt = f"""ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ì—ê²Œ ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ì•„ë˜ [ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì¶”ì²œí•˜ëŠ” ì¼ìë¦¬ ì¤‘ ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ê²ƒ í•˜ë‚˜ì˜ ì œëª©ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

#                         [ì •ë³´]
#                         {context}

#                         [ì§ˆë¬¸]
#                         {request.query}"""

#         chat_response = client.chat.completions.create(
#             model="gpt-4o-mini",
#             messages=[{"role": "user", "content": prompt}]
#         )
#         answer = chat_response.choices[0].message.content

#         # --- 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜ ---
#         return {"answer": answer, "jobs": top_5_jobs}
        
#     except Exception as e:
#         error_traceback = traceback.format_exc()
#         raise HTTPException(status_code=500, detail=error_traceback)

# --- RAG íŒŒì´í”„ë¼ì¸ ---
def run_rag_pipeline(user_id: UUID, query: str, k: int, exclude_ids: Optional[List[int]] = None, current_latitude: Optional[float] = None, current_longitude: Optional[float] = None) -> dict:
    # 1. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
    user_response = supabase.from_("users").select("*").eq("id", str(user_id)).single().execute()
    user_ctx = user_response.data
    if not user_ctx:
        raise HTTPException(status_code=404, detail="ì‚¬ìš©ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ğŸ‘‡ --- íˆìŠ¤í† ë¦¬ ì¡°íšŒ ë¡œì§ ì¶”ê°€ --- ğŸ‘‡
    history_response = supabase.from_("user_job_reviews").select("job_id, status").eq("user_id", str(user_id)).execute()
    user_history = history_response.data or []
    
    accepted_ids = {item['job_id'] for item in user_history if item['status'] in ['applied', 'completed', 'saved']}
    rejected_ids = {item['job_id'] for item in user_history if item['status'] in ['rejected']}
    # ğŸ‘† --- ë¡œì§ ì¶”ê°€ ë --- ğŸ‘†

    # 2. ì¿¼ë¦¬ ì„ë² ë”©
    embedding_response = client.embeddings.create(input=[query], model="text-embedding-3-small")
    query_embedding = embedding_response.data[0].embedding

    # 3. í›„ë³´êµ° ê²€ìƒ‰ (Retrieval)
    candidates_response = supabase.rpc('match_jobs', {'query_embedding': query_embedding, 'match_threshold': 0.3, 'match_count': 150}).execute()
    retrieved_jobs = candidates_response.data
    if not retrieved_jobs:
        return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ìš”ì²­ê³¼ ìœ ì‚¬í•œ ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}
    
    # ğŸ‘‡ --- ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•©ë‹ˆë‹¤ --- ğŸ‘‡
    # ì œì™¸í•  IDê°€ ìˆë‹¤ë©´, ì´ˆê¸° í›„ë³´êµ°ì—ì„œ í•„í„°ë§í•©ë‹ˆë‹¤.
    print(exclude_ids)
    if exclude_ids:
        print(f"--- ì œì™¸ ë¡œì§ ì‹¤í–‰! ì œì™¸í•  ID: {exclude_ids} ---")
        
        retrieved_jobs_before_filter = len(retrieved_jobs)
        retrieved_jobs = [job for job in retrieved_jobs if int(job['job_id']) not in exclude_ids]
        print(f"í•„í„°ë§ ì „: {retrieved_jobs_before_filter}ê°œ -> í•„í„°ë§ í›„: {len(retrieved_jobs)}ê°œ")
    
    if not retrieved_jobs:
        return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ë” ì´ìƒ ì¶”ì²œí•´ë“œë¦´ ë‹¤ë¥¸ ì†Œì¼ê±°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.", "jobs": []}
    # ğŸ‘† --- ìˆ˜ì • ë --- ğŸ‘†
    

    retrieved_ids = [job['job_id'] for job in retrieved_jobs]
    similarity_map = {job['job_id']: job['similarity'] for job in retrieved_jobs}
    
    full_candidates_response = supabase.from_("jobs").select("*").in_("job_id", retrieved_ids).execute()
    candidates = full_candidates_response.data

    # 4. í•„í„°ë§ ë° ì¬ì •ë ¬ (Filtering & Reranking)
    reranked_jobs = []
    
    # ì§€ì—­ë³„ ì„ê¸ˆ í†µê³„ ì‚¬ì „ ê³„ì‚°
    by_place: Dict[str, List[Dict[str, Any]]] = {}
    for c in candidates:
        by_place.setdefault(c.get("place",""), []).append(c)
    
    
    for job in candidates:
        # --- FactPack ìƒì„± ---
        # ê±°ë¦¬ ë° ì´ë™ ì‹œê°„
        
        # ğŸ‘‡ --- ê¸°ì¤€ ìœ„ì¹˜ ê²°ì • ë¡œì§ (ì‚¬ìš©ì ì œì•ˆ ë°˜ì˜) --- ğŸ‘‡
        if current_latitude is not None and current_longitude is not None:
            # ìš”ì²­ì— í˜„ì¬ ìœ„ì¹˜ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©
            base_lat = current_latitude
            base_lon = current_longitude
        else:
            # ìš”ì²­ì— í˜„ì¬ ìœ„ì¹˜ê°€ ì—†ìœ¼ë©´ DBì— ì €ì¥ëœ ì§‘ ì£¼ì†Œë¥¼ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©
            base_lat = user_ctx.get('home_latitude')
            base_lon = user_ctx.get('home_longitude')
        
        distance_km = haversine_km(base_lat, base_lon, job.get('job_latitude'), job.get('job_longitude'))
        # if base_lat is None or base_lon is None:
        #     # ê¸°ì¤€ì ì´ ì—†ìœ¼ë©´ ê±°ë¦¬ ê³„ì‚°ì„ ê±´ë„ˆëœë‹ˆë‹¤.
        #     # distance_km = float('inf')
        #     distance_km = haversine_km(user_ctx.get('home_latitude'), user_ctx.get('home_longitude'), job.get('job_latitude'), job.get('job_longitude'))
        # else:
        # --- ğŸ‘† ìˆ˜ì • ë --- ğŸ‘†
        
        
        # distance_km = haversine_km(user_ctx.get('home_latitude'), user_ctx.get('home_longitude'), job.get('job_latitude'), job.get('job_longitude'))
        travel_min = estimate_travel_min(distance_km)
        
        # ì‹œê°„ ì í•©ë„
        time_metrics = compute_time_overlap_metrics(
            user_ctx.get("availability_json", {}),
            job.get("work_days"),
            job.get("start_time"),
            job.get("end_time")
        )
        time_fit_score = time_metrics.get("time_fit", 0.0)
        
        # ì„ê¸ˆ ì •ê·œí™” ì ìˆ˜
        # pay_norm_score = compute_pay_norm(by_place.get(job.get("place"), []), job.get("hourly_wage"))
        pay_norm_score = 0.5 # ì„ì‹œê°’
        
        # ğŸ‘‡ --- íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì ìˆ˜ ì¶”ê°€ --- ğŸ‘‡
        history_score = 0
        if job['job_id'] in accepted_ids:
            history_score = 1.0 # ìˆ˜ë½í•œ ì¼ìë¦¬ëŠ” ê°€ì‚°ì 
        elif job['job_id'] in rejected_ids:
            history_score = -1.0 # ê±°ì ˆí•œ ì¼ìë¦¬ëŠ” í° ê°ì 
        # ğŸ‘† --- ë¡œì§ ì¶”ê°€ ë --- ğŸ‘†
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì˜ˆì‹œ)
        final_score = (
            similarity_map.get(job['job_id'], 0) * 0.5 + # ì˜ë¯¸ ìœ ì‚¬ë„ 50%
            (1 - (distance_km / 20) if distance_km <= 20 else 0) * 0.1 + # ê±°ë¦¬ 10%
            time_fit_score * 0.2 + # ì‹œê°„ ì í•©ë„ 20%
            pay_norm_score * 0.1 + # ì„ê¸ˆ 10%
            history_score * 0.1   # íˆìŠ¤í† ë¦¬ 10%
        )
        
        job['match_score'] = round(final_score, 4)
        job['distance_km'] = round(distance_km, 2)
        job['travel_min'] = travel_min
        job['time_fit'] = time_fit_score
        
        reranked_jobs.append(job)
        
    reranked_jobs.sort(key=lambda x: x.get('match_score', 0), reverse=True)
    top_k_jobs = reranked_jobs[:k]
    if not top_k_jobs:
        return {"answer": "ì¡°ê±´ì— ë§ëŠ” ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}
    
    for job in top_k_jobs:
        try:
            # ê° jobë§ˆë‹¤ LLMì— ë³´ë‚¼ ê°œë³„ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            prompt = build_prompt_for_reason(job, user_ctx, query)
            
            # ê° jobì— ëŒ€í•´ LLM í˜¸ì¶œ
            reason_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            reason_text = reason_response.choices[0].message.content
            
            # ìƒì„±ëœ ì´ìœ ë¥¼ job ê°ì²´ì— 'reason' í‚¤ë¡œ ì¶”ê°€
            job['reason'] = reason_text
        except Exception as e:
            print(f"Job ID {job.get('job_id')} ì´ìœ  ìƒì„± ì‹¤íŒ¨: {e}")
            # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì´ìœ (fallback)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            job['reason'] = generate_fallback_reason(job)

    # 5. ìµœì¢… ë‹µë³€ ìƒì„± (Generation)
    context = "\n\n".join([f"- ì œëª©: {job['title']}\n- ë‚´ìš©: {job['description']}" for job in top_k_jobs])
    # prompt = f"ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ì—ê²Œ ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ì•„ë˜ [ì •ë³´]'{context}'ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ [ì§ˆë¬¸] '{query}'ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    prompt = f"""
        ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì‚¬ìš©ìì—ê²Œ ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œ 'ì¡ìˆìœ¼'ì…ë‹ˆë‹¤.
        ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì•„ë˜ [ê²€ìƒ‰ëœ ì¼ìë¦¬ ì •ë³´]ì™€ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ì™œ ì´ ì¼ìë¦¬ë“¤ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€ë¥¼ ì„¤ëª…í•˜ëŠ” ê°œì¸í™”ëœ ì¶”ì²œ ë©”ì‹œì§€ë¥¼ ìƒˆë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

        [ê·œì¹™]
        1. ë”±ë”±í•œ ìš”ì•½ì´ ì•„ë‹Œ, ì‚¬ëŒì—ê²Œ ë§ì„ ê±°ëŠ” ë“¯í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        2. ê²€ìƒ‰ëœ ì •ë³´ ì¤‘ì—ì„œ ê°€ì¥ ì¶”ì²œ ì ìˆ˜ê°€ ë†’ì€ ì¼ìë¦¬ 1~2ê°œë¥¼ ì–¸ê¸‰í•˜ë©°, ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì—®ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        3. ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬(ì˜ˆ: 'ì¡°ìš©í•œ', 'ì»´í“¨í„°')ì„ ë‹µë³€ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.
        4. ìµœì¢… ë‹µë³€ì€ 2~3 ë¬¸ì¥ìœ¼ë¡œ ì™„ì„±í•˜ì„¸ìš”.

        [ê²€ìƒ‰ëœ ì¼ìë¦¬ ì •ë³´]
        {context}

        [ì§ˆë¬¸]
        {query}

        [ì¶”ì²œ ë©”ì‹œì§€]
        """
    chat_response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}])
    answer = chat_response.choices[0].message.content

    return {"answer": answer, "jobs": top_k_jobs}


# [AI ì¶”ì²œ]
@app.post("/api/recommend")
def recommend_jobs_text(request: RecommendRequest):
    """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ë°›ì•„ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        return run_rag_pipeline(request.user_id, request.query, 10, request.exclude_ids, request.current_latitude, request.current_longitude)
        # return run_rag_pipeline(request.user_id, request.query, 3)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/stt")
def speech_to_text(audio_file: UploadFile = File(...)):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # Whisper APIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” (íŒŒì¼ëª…, íŒŒì¼ë‚´ìš©, MIMEíƒ€ì…) íŠœí”Œ í˜•íƒœë¡œ ì „ë‹¬
        file_content = audio_file.file.read()
        mime_type = "audio/m4a" if audio_file.filename.lower().endswith('.m4a') else audio_file.content_type

        transcript_response = client.audio.transcriptions.create(
            model="whisper-1",
            file=(audio_file.filename, file_content, mime_type),
            response_format="text"
        )
        
        query_text = str(transcript_response).strip()
        print(f"ğŸ¤ Whisper STT ê²°ê³¼: \"{query_text}\"")

        # ë³€í™˜ëœ í…ìŠ¤íŠ¸ë§Œ JSON í˜•íƒœë¡œ ë°˜í™˜
        return {"text": query_text}

    except Exception as e:
        error_traceback = traceback.format_exc()
        raise HTTPException(status_code=500, detail=error_traceback)    


@app.post("/api/recommend-voice")
def recommend_jobs_voice(user_id: UUID = Form(...), audio_file: UploadFile = File(...), exclude_ids: Optional[str] = Form(None), current_latitude: Optional[float] = Form(None), current_longitude: Optional[float] = Form(None)):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„, RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # transcript = client.audio.transcriptions.create(model="whisper-1", file=audio_file.file, response_format="text")
         # ğŸ‘‡ --- ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•©ë‹ˆë‹¤ --- ğŸ‘‡
        # Whisper APIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” (íŒŒì¼ëª…, íŒŒì¼ë‚´ìš©) íŠœí”Œ í˜•íƒœë¡œ ì „ë‹¬
        transcript_response = client.audio.transcriptions.create(
            model="whisper-1",
            file=(audio_file.filename, audio_file.file.read()),
            response_format="text"
        )
        # ğŸ‘† --- ìˆ˜ì • ë --- ğŸ‘†
        
        # 2. ë°›ì€ ë¬¸ìì—´ì„ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        # exclude_ids = []
        if exclude_ids:
            # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ìˆ«ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            exclude_ids = [int(id_str) for id_str in exclude_ids.split(',')]


        # transcript_responseê°€ ì´ì œ ì‘ë‹µ ê°ì²´ì´ë¯€ë¡œ, ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
        # (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜, ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.)
        query_text = transcript_response.strip()
        print(f"ğŸ¤ Whisper STT ê²°ê³¼: \"{query_text}\"")
        print(f"ì œì™¸í•  ID: {exclude_ids}")
        return run_rag_pipeline(user_id, query_text, 5, exclude_ids, current_latitude, current_longitude)
    except Exception as e:
        error_traceback = traceback.format_exc()
        raise HTTPException(status_code=500, detail=error_traceback)
    
@app.post("/api/engagements")
def record_engagement(request: EngagementRequest):
    """ì‚¬ìš©ìì˜ ì¼ìë¦¬ ìƒí˜¸ì‘ìš©(ì§€ì›, ì €ì¥, ê±°ì ˆ ë“±)ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    
    # status ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸
    valid_statuses = ['saved', 'applied', 'completed', 'cancelled', 'rejected', 'dismissed']
    if request.status not in valid_statuses:
        raise HTTPException(status_code=422, detail="ìœ íš¨í•˜ì§€ ì•Šì€ status ê°’ì…ë‹ˆë‹¤.")

    try:
        # upsertë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë¡ì„ ìƒì„±í•˜ê±°ë‚˜ ê°±ì‹ í•©ë‹ˆë‹¤.
        # í•œ ì‚¬ìš©ìê°€ í•œ ì¼ìë¦¬ì— ëŒ€í•´ ê°€ì§€ëŠ” ìƒíƒœëŠ” í•˜ë‚˜ë¿ì…ë‹ˆë‹¤.
        response = supabase.from_("user_job_reviews").upsert({
            "user_id": str(request.user_id),
            "job_id": request.job_id,
            "status": request.status
        }, on_conflict="user_id, job_id").execute() # user_idì™€ job_idê°€ ê²¹ì¹˜ë©´ update

        return {"message": f"'{request.status}' ìƒíƒœê°€ ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í–‰ë™ ê¸°ë¡ ì‹¤íŒ¨: {str(e)}")
    
@app.get("/api/users/{user_id}/profile")
def get_user_profile(user_id: UUID):
    """íŠ¹ì • ì‚¬ìš©ìì˜ í”„ë¡œí•„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        # users í…Œì´ë¸”ì—ì„œ idê°€ ì¼ì¹˜í•˜ëŠ” ì‚¬ìš©ìë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        response = supabase.from_("users").select("*").eq("id", str(user_id)).single().execute()
        
        if not response.data:
            raise HTTPException(status_code=404, detail="í”„ë¡œí•„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        return response.data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í”„ë¡œí•„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    
@app.put("/api/users/{user_id}/profile")
def update_user_profile(user_id: UUID, profile_update: UserProfileUpdate):
    """íŠ¹ì • ì‚¬ìš©ìì˜ í”„ë¡œí•„ ì •ë³´ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."""
    try:
        # Pydantic ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ê³ , None ê°’ì€ ì œì™¸í•˜ì—¬
        # ì‚¬ìš©ìê°€ ë³´ë‚¸ í•„ë“œë§Œ ì—…ë°ì´íŠ¸í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        update_data = profile_update.model_dump(exclude_unset=True)
        
        if not update_data:
            raise HTTPException(status_code=400, detail="ìˆ˜ì •í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

        response = supabase.from_("users").update(update_data).eq("id", str(user_id)).execute()
        
        if not response.data:
            raise HTTPException(status_code=404, detail="í”„ë¡œí•„ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        return response.data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í”„ë¡œí•„ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")