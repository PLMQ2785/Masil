import hashlib
import json
import math
import os
import traceback
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

import numpy as np
import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from pydantic import BaseModel, Field
from supabase import Client, create_client

# --- 1. ì´ˆê¸°í™” ---
load_dotenv()

# Supabase ë° OpenAI í´ë¼ì´ì–¸íŠ¸
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# FastAPI ì•±
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "https://localhost:5173", # https ë¡œì»¬í˜¸ìŠ¤íŠ¸
        "http://192.168.68.67:5173", # http IP ì£¼ì†Œ
        "https://192.168.68.67:5173", # ğŸ‘ˆ ì´ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”.
        "https://jobis.ngrok.app",
        "https://jobisbe.ngrok.app"
        ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 2. Pydantic ë°ì´í„° ëª¨ë¸ ---
class Job(BaseModel):
    title: str
    participants: Optional[int] = None
    hourly_wage: int
    place: str
    address: Optional[str] = None
    work_days: Optional[str] = Field(None, max_length=7)
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    client: Optional[str] = None
    description: Optional[str] = None
    job_latitude: float
    job_longitude: float

class Review(BaseModel):
    user_id: UUID
    rating: int = Field(..., ge=1, le=5)
    review_text: Optional[str] = None
    status: str

class RecommendRequest(BaseModel):
    user_id: UUID
    query: str

class ApplyRequest(BaseModel):
    user_id: UUID
    
class SessionUpdateRequest(BaseModel):
    user_id: UUID
    session_id: UUID

# --- 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (AI-1, AI-2 ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê°€ì ¸ì˜´) ---
WEEKDAYS = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]

def haversine_km(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371.0088
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = phi2 - phi1
    dlmb = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlmb / 2) ** 2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return R * c

# (ê¸°íƒ€ compute_time_overlap, llm_enrich_batch ë“± í•„ìš”í•œ ìœ í‹¸ í•¨ìˆ˜ë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤)

# --- 4. API ì—”ë“œí¬ì¸íŠ¸ ---

# [Jobs CRUD]
@app.post("/api/jobs")
def create_job(job: Job):
    text_to_embed = f"ì œëª©: {job.title}\në‚´ìš©: {job.description}\nì¥ì†Œ: {job.place}\ní´ë¼ì´ì–¸íŠ¸: {job.client}"
    try:
        embedding_response = client.embeddings.create(input=[text_to_embed], model="text-embedding-3-small")
        embedding_vector = embedding_response.data[0].embedding
        job_data = job.model_dump()
        job_data["embedding"] = embedding_vector
        response = supabase.from_("jobs").insert(job_data).execute()
        return response.data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.get("/api/jobs")
def get_jobs(user_id: Optional[UUID] = None, limit: int = 100):
    """
    user_id ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì¶”ì²œ ëª©ë¡ì„ ë°˜í™˜í•˜ê³ ,
    ì—†ìœ¼ë©´ ê´€ë¦¬ì í˜ì´ì§€ë¥¼ ìœ„í•œ ì „ì²´ ì¼ìë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # --- 1. ê°œì¸í™” ì¶”ì²œ ë¡œì§ (user_idê°€ ìˆì„ ê²½ìš°) ---
        if user_id:
            # 1a. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (ê¸°ì¤€ ìœ„ì¹˜, ì„ í˜¸ ì§ë¬´ ë“±)
            user_response = supabase.from_("users").select(
                "home_latitude, home_longitude, preferred_jobs"
            ).eq("id", str(user_id)).single().execute()
            
            user_profile = user_response.data
            if not user_profile or user_profile.get("home_latitude") is None:
                raise HTTPException(status_code=404, detail="ì‚¬ìš©ì í”„ë¡œí•„ ë˜ëŠ” ê¸°ì¤€ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

            user_lat = user_profile["home_latitude"]
            user_lon = user_profile["home_longitude"]
            preferred_jobs = user_profile.get("preferred_jobs", [])

            # 1b. ì‚¬ìš©ìì˜ ê¸°ì¤€ ìœ„ì¹˜ ì£¼ë³€ ì¼ìë¦¬ ê²€ìƒ‰ (1ì°¨ í•„í„°ë§)
            nearby_jobs_response = supabase.rpc('nearby_jobs_full', {
                'user_lat': user_lat,
                'user_lon': user_lon,
                'radius_meters': 10000, # 10km ë°˜ê²½
                'result_limit': limit
            }).execute()
            
            nearby_jobs_data = nearby_jobs_response.data
            if not nearby_jobs_data:
                return []

            # 1c. ì¬ì •ë ¬ (Reranking): ì„ í˜¸ ì§ë¬´ì™€ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            recommended_jobs = []
            for job in nearby_jobs_data:
                title = job.get("title", "")
                
                # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°: ì„ í˜¸ ì§ë¬´ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ë©´ 1ì ì”© ì¶”ê°€
                preference_score = 0
                if preferred_jobs:
                    for pref in preferred_jobs:
                        if pref in title:
                            preference_score += 1
                
                # ê±°ë¦¬ ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ)
                distance = haversine_km(user_lat, user_lon, job['job_latitude'], job['job_longitude'])
                distance_score = 1 - (distance / 10) if distance <= 10 else 0

                # ìµœì¢… ì ìˆ˜ (ì„ í˜¸ë„ 50%, ê±°ë¦¬ 50%)
                job['match_score'] = round((preference_score * 0.5) + (distance_score * 0.5), 4)
                recommended_jobs.append(job)

            # ìµœì¢… ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
            recommended_jobs.sort(key=lambda x: x['match_score'], reverse=True)

            return recommended_jobs

        # --- 2. ì „ì²´ ì¡°íšŒ ë¡œì§ (user_idê°€ ì—†ì„ ê²½ìš°) ---
        else:
            response = supabase.from_("jobs").select("*").order("created_at", desc=True).limit(limit).execute()
            return response.data

    except Exception as e:
        error_traceback = traceback.format_exc()
        raise HTTPException(status_code=500, detail=error_traceback)


@app.get("/api/jobs/{job_id}")
def get_job_by_id(job_id: int):
    try:
        response = supabase.from_("jobs").select("*").eq("job_id", job_id).single().execute()
        return response.data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ID {job_id} ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.put("/api/jobs/{job_id}")
def update_job(job_id: int, job: Job):
    text_to_embed = f"ì œëª©: {job.title}\në‚´ìš©: {job.description}\nì¥ì†Œ: {job.place}\ní´ë¼ì´ì–¸íŠ¸: {job.client}"
    try:
        embedding_response = client.embeddings.create(input=[text_to_embed], model="text-embedding-3-small")
        embedding_vector = embedding_response.data[0].embedding
        job_data = job.model_dump()
        job_data["embedding"] = embedding_vector
        job_data["updated_at"] = "now()"
        response = supabase.from_("jobs").update(job_data).eq("job_id", job_id).execute()
        return response.data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")

@app.delete("/api/jobs/{job_id}")
def delete_job(job_id: int):
    try:
        response = supabase.from_("jobs").delete().eq("job_id", job_id).execute()
        if not response.data:
            raise HTTPException(status_code=404, detail=f"ID {job_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"message": f"ID {job_id}ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

# [ì§€ì›í•˜ê¸°]
@app.post("/api/jobs/{job_id}/apply")
def apply_for_job(job_id: int, request: ApplyRequest):
    try:
        # 1. ì§€ì› ë§ˆê° ì—¬ë¶€ í™•ì¸ (ê¸°ì¡´ê³¼ ë™ì¼)
        job_response = supabase.from_("jobs").select("participants, current_participants").eq("job_id", job_id).single().execute()
        job = job_response.data
        if not job:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì¼ìë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if job.get('participants') is not None and job.get('current_participants', 0) >= job.get('participants'):
            raise HTTPException(status_code=400, detail="ëª¨ì§‘ì´ ë§ˆê°ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 2. ì‚¬ìš©ìì˜ ì§€ì› ê¸°ë¡ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
        review_data = {"job_id": job_id, "user_id": str(request.user_id), "status": "applied"}
        supabase.from_("user_job_reviews").upsert(review_data).execute()
        
        # 3. DB í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì§€ì›ì ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ 1 ì¦ê°€
        supabase.rpc('increment_applicants', {'job_id_to_update': job_id}).execute()

        return {"message": "ì§€ì›ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì§€ì› ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


# [Reviews CRUD]
@app.post("/api/jobs/{job_id}/reviews")
def create_review_for_job(job_id: int, review: Review):
    try:
        review_data = review.model_dump()
        review_data["job_id"] = job_id
        review_data["user_id"] = str(review.user_id)
        supabase.from_("user_job_reviews").insert(review_data).execute()
        
        agg_response = supabase.from_("user_job_reviews").select("rating", count="exact").eq("job_id", job_id).execute()
        ratings = [item['rating'] for item in agg_response.data if item.get('rating') is not None]
        new_review_count = agg_response.count
        new_avg_rating = sum(ratings) / len(ratings) if ratings else 0

        supabase.from_("jobs").update({
            # "average_rating": new_avg_rating,
            # "review_count": new_review_count
        }).eq("job_id", job_id).execute()
        
        return {"message": "ë¦¬ë·°ê°€ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¦¬ë·° ë“±ë¡ ì‹¤íŒ¨: {str(e)}")

@app.get("/api/jobs/{job_id}/reviews")
def get_reviews_for_job(job_id: int):
    try:
        response = supabase.from_("user_job_reviews").select("*").eq("job_id", job_id).execute()
        return response.data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¦¬ë·° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# [Users Utility]
class SessionUpdateRequest(BaseModel):
    user_id: UUID
    session_id: UUID

@app.post("/api/users/update-session")
def update_user_session(request: SessionUpdateRequest):
    try:
        response = supabase.from_("users").update({
            "latest_session_id": str(request.session_id)
        }).eq("id", str(request.user_id)).execute()
        if not response.data:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"message": "ì„¸ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")


# [Geocoding Utility]
@app.get("/api/geocode")
def geocode_address(address: str = Query(..., min_length=1)):
    api_key_id = os.getenv('NAVER_API_KEY_ID')
    api_key = os.getenv('NAVER_API_KEY')
    if not api_key_id or not api_key: raise HTTPException(status_code=500, detail="API í‚¤ê°€ ì„œë²„ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    url = f"https://maps.apigw.ntruss.com/map-geocode/v2/geocode?query={address}"
    headers = {"X-NCP-APIGW-API-KEY-ID": api_key_id, "X-NCP-APIGW-API-KEY": api_key}
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        if data.get("status") == "OK" and data.get("addresses"):
            coords = data["addresses"][0]
            return {"latitude": float(coords["y"]), "longitude": float(coords["x"])}
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì£¼ì†Œì˜ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Naver API í†µì‹  ì˜¤ë¥˜: {str(e)}")


# [Recommendation RAG API]
@app.post("/api/recommend")
def recommend_jobs(request: RecommendRequest):
    """(ë™ê¸° ìµœì¢…ë³¸) ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ì¶”ì²œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # --- 1ë‹¨ê³„: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ---
        user_response = supabase.from_("users").select("*").eq("id", str(request.user_id)).single().execute()
        user_ctx = user_response.data
        if not user_ctx:
            raise HTTPException(status_code=404, detail="ì‚¬ìš©ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # --- 2ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ---
        embedding_response = client.embeddings.create(input=[request.query], model="text-embedding-3-small")
        query_embedding = embedding_response.data[0].embedding

        # --- 3ë‹¨ê³„: í›„ë³´êµ° ê²€ìƒ‰ (Retrieval) ---
        candidates_response = supabase.rpc('match_jobs', {
            'query_embedding': query_embedding,
            'match_threshold': 0.3, # ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì´ ê°’ì„ íŠœë‹í•´ì•¼ í•©ë‹ˆë‹¤.
            'match_count': 50
        }).execute()
        
        retrieved_jobs = candidates_response.data
        if not retrieved_jobs:
            return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ìš”ì²­ê³¼ ìœ ì‚¬í•œ ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}

        retrieved_ids = [job['job_id'] for job in retrieved_jobs]
        similarity_map = {job['job_id']: job['similarity'] for job in retrieved_jobs}
        
        full_candidates_response = supabase.from_("jobs").select("*").in_("job_id", retrieved_ids).execute()
        candidates = full_candidates_response.data

        # --- 4ë‹¨ê³„: í•„í„°ë§ ë° ì¬ì •ë ¬ (Filtering & Reranking) ---
        reranked_jobs = []
        for job in candidates:
            # ê±°ë¦¬ ê³„ì‚°
            distance_km = haversine_km(
                user_ctx.get('home_latitude'), user_ctx.get('home_longitude'),
                job.get('job_latitude'), job.get('job_longitude')
            )
            # TODO: AI-1ì˜ ìƒì„¸ ê³„ì‚° ë¡œì§ (ì‹œê°„ ê²¹ì¹¨, ì„ê¸ˆ ì •ê·œí™” ë“±)ì„ ì—¬ê¸°ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì˜ˆì‹œ: ì˜ë¯¸ìœ ì‚¬ë„ 70%, ê±°ë¦¬ 30%)
            distance_score = 1 - (distance_km / 20) if distance_km <= 20 else 0 # 20kmë¥¼ ìµœëŒ€ ê±°ë¦¬ë¡œ ê°€ì •
            match_score = similarity_map.get(job['job_id'], 0) * 0.7 + distance_score * 0.3
            
            job['match_score'] = round(match_score, 4)
            job['distance_km'] = round(distance_km, 2)
            reranked_jobs.append(job)
            
        reranked_jobs.sort(key=lambda x: x.get('match_score', 0), reverse=True)
        top_5_jobs = reranked_jobs[:5]

        if not top_5_jobs:
            return {"answer": "ì¡°ê±´ì— ë§ëŠ” ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}

        # --- 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„± (Generation) ---
        context = "\n\n".join([f"- ì œëª©: {job['title']} (ID: {job['job_id']})\n- ë‚´ìš©: {job['description']}" for job in top_5_jobs])
        prompt = f"""ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ì—ê²Œ ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ì•„ë˜ [ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì¶”ì²œí•˜ëŠ” ì¼ìë¦¬ ì¤‘ ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ê²ƒ í•˜ë‚˜ì˜ ì œëª©ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

                        [ì •ë³´]
                        {context}

                        [ì§ˆë¬¸]
                        {request.query}"""

        chat_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        answer = chat_response.choices[0].message.content

        # --- 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜ ---
        return {"answer": answer, "jobs": top_5_jobs}
        
    except Exception as e:
        error_traceback = traceback.format_exc()
        raise HTTPException(status_code=500, detail=error_traceback)

# --- RAG íŒŒì´í”„ë¼ì¸ ---
def run_rag_pipeline(user_id: UUID, query: str) -> dict:
    # 1. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
    user_response = supabase.from_("users").select("*").eq("id", str(user_id)).single().execute()
    user_ctx = user_response.data
    if not user_ctx:
        raise HTTPException(status_code=404, detail="ì‚¬ìš©ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2. ì¿¼ë¦¬ ì„ë² ë”©
    embedding_response = client.embeddings.create(input=[query], model="text-embedding-3-small")
    query_embedding = embedding_response.data[0].embedding

    # 3. í›„ë³´êµ° ê²€ìƒ‰ (Retrieval)
    candidates_response = supabase.rpc('match_jobs', {'query_embedding': query_embedding, 'match_threshold': 0.3, 'match_count': 50}).execute()
    retrieved_jobs = candidates_response.data
    if not retrieved_jobs:
        return {"answer": "ì£„ì†¡í•˜ì§€ë§Œ, ìš”ì²­ê³¼ ìœ ì‚¬í•œ ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}

    retrieved_ids = [job['job_id'] for job in retrieved_jobs]
    similarity_map = {job['job_id']: job['similarity'] for job in retrieved_jobs}
    
    full_candidates_response = supabase.from_("jobs").select("*").in_("job_id", retrieved_ids).execute()
    candidates = full_candidates_response.data

    # 4. í•„í„°ë§ ë° ì¬ì •ë ¬ (Filtering & Reranking)
    reranked_jobs = []
    for job in candidates:
        distance_km = haversine_km(user_ctx.get('home_latitude'), user_ctx.get('home_longitude'), job.get('job_latitude'), job.get('job_longitude'))
        distance_score = 1 - (distance_km / 20) if distance_km <= 20 else 0
        match_score = similarity_map.get(job['job_id'], 0) * 0.7 + distance_score * 0.3
        job['match_score'] = round(match_score, 4)
        job['distance_km'] = round(distance_km, 2)
        reranked_jobs.append(job)
        
    reranked_jobs.sort(key=lambda x: x.get('match_score', 0), reverse=True)
    top_5_jobs = reranked_jobs[:5]
    if not top_5_jobs:
        return {"answer": "ì¡°ê±´ì— ë§ëŠ” ì†Œì¼ê±°ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "jobs": []}

    # 5. ìµœì¢… ë‹µë³€ ìƒì„± (Generation)
    context = "\n\n".join([f"- ì œëª©: {job['title']}\n- ë‚´ìš©: {job['description']}" for job in top_5_jobs])
    prompt = f"ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ì—ê²Œ ì¼ìë¦¬ë¥¼ ì¶”ì²œí•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ì•„ë˜ [ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ [ì§ˆë¬¸] '{query}'ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    chat_response = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}])
    answer = chat_response.choices[0].message.content

    return {"answer": answer, "jobs": top_5_jobs}


# [AI ì¶”ì²œ]
@app.post("/api/recommend")
def recommend_jobs_text(request: RecommendRequest):
    """í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ë°›ì•„ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        return run_rag_pipeline(request.user_id, request.query)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/recommend-voice")
def recommend_jobs_voice(user_id: UUID = Form(...), audio_file: UploadFile = File(...)):
    """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„, RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # transcript = client.audio.transcriptions.create(model="whisper-1", file=audio_file.file, response_format="text")
         # ğŸ‘‡ --- ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•©ë‹ˆë‹¤ --- ğŸ‘‡
        # Whisper APIê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” (íŒŒì¼ëª…, íŒŒì¼ë‚´ìš©) íŠœí”Œ í˜•íƒœë¡œ ì „ë‹¬
        transcript_response = client.audio.transcriptions.create(
            model="whisper-1",
            file=(audio_file.filename, audio_file.file.read()),
            response_format="text"
        )
        # ğŸ‘† --- ìˆ˜ì • ë --- ğŸ‘†

        # transcript_responseê°€ ì´ì œ ì‘ë‹µ ê°ì²´ì´ë¯€ë¡œ, ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
        # (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‚˜, ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.)
        query_text = transcript_response.strip()
        print(f"ğŸ¤ Whisper STT ê²°ê³¼: \"{query_text}\"")
        return run_rag_pipeline(user_id, query_text)
    except Exception as e:
        error_traceback = traceback.format_exc()
        raise HTTPException(status_code=500, detail=error_traceback)